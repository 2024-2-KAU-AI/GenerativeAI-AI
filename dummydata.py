dummydata=[
        {
            "질문": "생성형 AI가 무엇인가요?",
            "생성형 AI 교수님 답변": "생성형 AI(Generative AI)는 주어진 데이터를 기반으로 새로운 콘텐츠를 만들어내는 인공지능 기술입니다. "
                             "예를 들어, ChatGPT와 같은 언어 모델은 질문을 입력받아 적절한 답변을 생성하고, DALL·E는 텍스트 설명을 바탕으로 이미지를 생성합니다. "
                             "이 기술은 글쓰기, 그림 그리기, 음악 작곡 등 창작 과정에 참여할 수 있는 AI의 한 형태로, "
                             "대규모 데이터와 딥러닝 모델을 활용해 패턴을 학습하고 이를 바탕으로 새로운 결과물을 만들어냅니다. "
                             "최근에는 교육, 예술, 의료, 게임 등 다양한 분야에서 활용되고 있습니다."
        },
        {
            "질문": "RAG 시스템의 기본 파이프라인은 어떻게 구성되나요?",
            "생성형 AI 교수님 답변": "RAG의 파이프라인은 크게 4단계로 나뉩니다. "
                             "예를 들어, 'AI 기술의 최근 트렌드는 무엇인가요?'라는 질문이 들어온다고 가정해 봅시다."
                             "\n1. 문서를 청크로 나눕니다. AI 기술 관련 문서들을 짧고 이해하기 쉬운 단위로 분리합니다."
                             "\n2. 각 청크를 임베딩으로 변환합니다. 즉, 각 청크를 숫자 벡터로 표현합니다."
                             "\n3. 쿼리와 가장 관련 있는 청크를 검색합니다. AI 기술 트렌드와 관련된 문서를 빠르게 찾습니다."
                             "\n4. 검색된 내용을 바탕으로 LLM이 답변을 생성합니다. 결과적으로, AI 트렌드에 대한 최신 정보를 포함한 답변이 제공됩니다."
        },
        {
            "질문": "문서를 청크로 나누는 방법에는 어떤 것들이 있나요?",
            "생성형 AI 교수님 답변": "문서를 청크로 나누는 방법은 여러 가지가 있습니다. 예를 들어, 책 한 권을 다루는 경우를 생각해 봅시다."
                             "\n- 기본적으로 일정한 길이로 나누는 방법: 책의 내용을 200자씩 잘라서 청크로 나누는 방식입니다. "
                             "하지만 문장이 중간에 끊길 수 있다는 단점이 있죠."
                             "\n- 문서 기반 청킹: 챕터나 절 단위로 나누는 방법입니다. 'AI의 역사'라는 챕터는 하나의 청크가 되고, 'AI의 현재'는 또 다른 청크가 됩니다."
                             "\n- 의미 기반 청킹: 관련 문장들끼리 묶어서 청크를 만듭니다. 예를 들어, 'AI의 역사' 안에서도 관련된 문장들을 하나로 묶어 청크를 생성하는 방식입니다. "
                             "이렇게 하면 문맥을 더 잘 이해할 수 있습니다."
        },
        {
            "질문": "text embedding의 원리가 뭔가요?",
            "생성형 AI 교수님 답변": "Text Embedding은 텍스트를 고차원 벡터 공간으로 변환하는 기술로, 각 텍스트의 의미적 특성을 수치로 표현하는 과정입니다. "
                             "예를 들어, '고양이'와 '개'라는 단어는 비슷한 맥락에서 사용되기 때문에 벡터 공간에서 서로 가까운 위치에 매핑됩니다."
                             "\n\n이 과정은 주로 'Encoder'라는 신경망 모델을 통해 이루어지며, 텍스트 청크를 입력으로 받아 벡터로 변환합니다. "
                             "변환된 벡터 간의 유사도를 계산하기 위해 코사인 유사도(cosine similarity)가 주로 사용됩니다. "
                             "예를 들어, '강아지가 공원에서 뛰고 있다'와 '개의 활동 장소는 공원이다'는 의미적으로 비슷하기 때문에 벡터 간 유사도가 높습니다."
                             "\n\nText Embedding은 대규모 데이터셋에서 학습한 사전 학습 모델(BERT, RoBERTa 등)이나 Contrastive Learning(대조 학습) 기법을 통해 생성됩니다. "
                             "이를 통해 텍스트 간의 의미를 더 잘 이해하고 응용할 수 있게 됩니다. "
                             "예를 들어, 검색 엔진에서 사용자의 질문과 데이터베이스 문서 간의 연관성을 파악하거나, 문장 간의 유사도를 분석하는 데 활용됩니다."
        }
]